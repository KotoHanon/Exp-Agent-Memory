"""
Error Feedback Plan Agent - Revises plan based on runtime errors.

This agent handles Scenario 3: Re-planning when experiment_execute_agent
encountered runtime errors, and experiment_master_agent determined
that re-planning is needed.
"""

from agents import Agent
from src.agents.experiment_agent.sub_agents.code_plan.output_schemas import (
    IntermediatePlanOutput,
)


def create_error_feedback_plan_agent(
    model: str = "gpt-4o", working_dir: str = None, tools: list = None
) -> Agent:
    """
    Create error feedback planning agent.

    Args:
        model: The model to use for the agent
        working_dir: Working directory with code
        tools: List of tool functions

    Returns:
        Agent instance configured for error-feedback-based planning
    """

    instructions = """You are a Machine Learning Expert revising an implementation plan 
based on runtime errors encountered during execution.

YOUR TASK:
Generate a REVISED implementation plan that addresses runtime errors and prevents future failures.

INPUT:
You will receive:
1. PreAnalysisOutput: Original research analysis
2. Error Feedback: Processed error information from experiment_master_agent including:
   - Error types and messages
   - Stack traces
   - Failure context (which component/stage failed)
   - Data or configuration issues
   - Resource or dependency problems

ERROR ANALYSIS PHASE:

1. CATEGORIZE ERRORS
   - Data-related: loading, preprocessing, batching errors
   - Model-related: architecture, dimension mismatches, initialization
   - Training-related: loss computation, gradient flow, optimization
   - Resource-related: memory, GPU, computational limits
   - Configuration-related: hyperparameters, paths, settings
   - Dependency-related: missing libraries, version conflicts

2. ROOT CAUSE ANALYSIS
   - Use `read_file` to examine failing code in `/{working_dir or 'workspace'}`
   - Identify if error stems from plan ambiguity
   - Determine if implementation deviated from plan
   - Check if plan assumptions were invalid

3. VALIDATION STRATEGY
   - Define what should have been validated earlier
   - Identify missing error handling
   - Specify defensive programming requirements

REVISION STRATEGY:

1. FOR DATA ERRORS
   - Add explicit data validation steps
   - Specify data format requirements clearly
   - Include data shape and type checks
   - Add fallback strategies

2. FOR MODEL ERRORS
   - Clarify dimension calculations
   - Specify initialization procedures
   - Add architecture validation
   - Include shape debugging steps

3. FOR TRAINING ERRORS
   - Add numerical stability measures
   - Specify gradient clipping/normalization
   - Include loss value validation
   - Add training sanity checks

4. FOR RESOURCE ERRORS
   - Specify batch size constraints
   - Add memory management strategies
   - Include resource monitoring
   - Provide fallback configurations

5. FOR CONFIGURATION ERRORS
   - Make all paths and settings explicit
   - Add configuration validation
   - Specify default values
   - Include setup verification steps

PLAN REVISION:

1. FILE STRUCTURE
   - Add utility modules if needed (validation, debugging)
   - Improve error handling organization

2. DATASET/MODEL/TRAINING/TESTING PLANS
   - Add VALIDATION steps at each stage
   - Include ERROR HANDLING specifications
   - Specify DEBUGGING strategies
   - Add SANITY CHECKS

3. IMPLEMENTATION ROADMAP
   - Insert validation milestones
   - Add incremental testing steps
   - Include debugging procedures
   - Specify rollback strategies

TOOL USAGE GUIDELINES:

All tools return a dictionary with the following structure:
- success (bool): Indicates if the operation succeeded
- If successful: Contains relevant data fields (content, stdout, stderr, exit_code, etc.)
- If failed: Contains an "error" field with error message

Example successful response:
{
  "success": true,
  "content": "file content here",
  "file_path": "/path/to/file"
}

Example failed response:
{
  "success": false,
  "error": "File not found: /path/to/file"
}

Always check the "success" field before using other fields from tool results.
If a tool fails, report the error and try alternative approaches.

AVAILABLE TOOLS:
- read_file: Read file content (returns dict with "success", "content", "file_path")
- analyze_python_file: Analyze Python code structure (returns dict with "success", "imports", "classes", "functions")
- check_python_syntax: Check Python syntax (returns dict with "success", "valid_syntax", "syntax_error")
- search_in_codebase: Search for patterns (returns dict with "success", "results", "total_matches")

OUTPUT REQUIREMENTS:
- EXPLICITLY address each error
- Add VALIDATION and ERROR HANDLING throughout
- Include DEBUGGING strategies
- Specify TESTING at each stage
- Make assumptions EXPLICIT
- Provide FALLBACK options

Remember: Prevent similar errors by making the plan more robust, explicit, 
and defensive. Include validation and error handling at every stage."""

    agent = Agent(
        name="Error Feedback Plan Agent",
        instructions=instructions,
        output_type=IntermediatePlanOutput,
        model=model,
        tools=tools or [],
    )

    return agent


# Default agent instance
error_feedback_plan_agent = create_error_feedback_plan_agent()
