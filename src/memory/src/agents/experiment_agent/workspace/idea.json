{
    "idea": {
        "title": "Symmetry-Constrained Neural-Symbolic Hybrid Models for Interpretable and Physically-Grounded Machine-Learned Force Fields",
        "description": "A hybrid neural-symbolic model that integrates symmetry-respecting equivariant GNNs, a symbolic pool of physically-motivated terms, and sparse, interpretable neural selection/regression in a unified architecture. The model aims to discover explicit, symbolic, symmetry-respecting interatomic potentials that are both accurate and human-understandable.",
        "key_innovations": [
            "Neural-symbolic integration: Blends data-driven neural inference and explainable analytical physics simultaneously",
            "Physics-motif library: Pool of candidate analytical potential terms based on physics/chemistry priors",
            "Adaptive, environment-dependent selection: Neural weights can be atom- or environment-specific",
            "Equivariance enforcement: All terms are constructed to be rotationally, translationally, and permutation equivariant/invariant"
        ],
        "methodology": {
            "symbolic_term_generator": "Builds a pool of candidate analytical potential terms (e.g., pair distances, angles, dihedrals)",
            "equivariant_gnn": "EquiformerV2 backbone maps 3D structure to node/edge embeddings",
            "neural_selection": "Neural attention/gating head combines symbolic pool and GNN features with sparsity regularization",
            "energy_model": "E = sum_{i,j,k} alpha_k(h_i, h_j, e_ij) * T_k(r_ij, ...)",
            "force_calculation": "F_i = -∂E/∂r_i (analytic gradients)"
        },
        "expected_outcomes": [
            "Competitive accuracy with state-of-the-art black-box equivariant GNNs, with far greater interpretability",
            "Human-readable force field formulas, potentially revealing new physico-chemical insights",
            "Improved generalization and data efficiency in low-resource and out-of-distribution regimes",
            "Human-in-the-loop capability for scientific hypothesis generation"
        ]
    },
            "reference_papers": [
                "EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations",
                "Equiformer: Equivariant graph attention transformer for 3d atomistic graphs",
                "Neural message passing for quantum chemistry",
                "Spherical CNNs"
  ]
}
